{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand Gesture With Recurrent Neural Network\n",
    "#### In this file we will see the TensorFlow-Keras implementation of two recurrent neural networks for the recognition of dynamic hand gestures. We will use American Sign Language (ASL) Dataset for training and testing of our model. We will train the following two bidirectional recurrent neural networkd models.\n",
    "#### a) Bidirectional Gated Recurrent Unit (Bi-GRU)\n",
    "#### b) Bidirectional Long Short Term Memory (Bi-LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctH5iw5tCmsZ"
   },
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "from keras.layers import Layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU, Dense, Dropout, Bidirectional, Input, TimeDistributed, CuDNNLSTM, CuDNNGRU\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "import pickle\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNN models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 200\n",
    "n_classes = 30\n",
    "dropout = 0.2\n",
    "attributes = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------\n",
    "# Creating GRU Model\n",
    "#--------------------------------------------------------------------\n",
    "def build_gru_model(units, attributes, dropout, n_classes):\n",
    "    # Functional API based model\n",
    "    inputs = Input(shape=(units, attributes))\n",
    "    gru1 = Bidirectional(GRU(units, activation='tanh', dropout=dropout, return_sequences=True))(inputs)\n",
    "    gru2 = Bidirectional(GRU(units, activation='tanh', dropout=dropout))(gru1)\n",
    "    output = Dense(n_classes, activation='softmax')(gru2)\n",
    "    model = Model(inputs, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------\n",
    "# Creating LSTM Model\n",
    "#--------------------------------------------------------------------\n",
    "def build_lstm_model(units, attributes, dropout, n_classes):\n",
    "    # Functional API based model\n",
    "    inputs = Input(shape=(units, attributes))\n",
    "    gru1 = Bidirectional(LSTM(units, activation='tanh', dropout=dropout, return_sequences=True))(inputs)\n",
    "    gru2 = Bidirectional(LSTM(units, activation='tanh', dropout=dropout))(gru1)\n",
    "    output = Dense(n_classes, activation='softmax')(gru2)\n",
    "    model = Model(inputs, output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_gru_model(units, attributes, dropout, n_classes)\n",
    "# model = build_lstm_model(units, attributes, dropout, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    '''\n",
    "    Returns train(x_train) and test(x_test) hand gesture sequences and \n",
    "    their associated labels (y_train), (y_test) respectively\n",
    "    '''\n",
    "    file = open(path, 'rb')\n",
    "    data = pickle.load(file, encoding='latin1')  # <<---- change to 'latin1' to 'utf8' if the data does not load\n",
    "    file.close()\n",
    "    return data['x_train'], data['x_test'], data['y_train'], data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# load data from pickle file\n",
    "#----------------------------------------------------------------------\n",
    "pickle_file_path = 'saved_datasets_pickle/asl.pckle'\n",
    "x_train, x_test, y_train, y_test = load_data(path=pickle_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Data Shuffle\n",
    "#----------------------------------------------------------------------  \n",
    "x_train, y_train= shuffle(x_train, y_train, random_state= 10)\n",
    "x_test, y_test= shuffle(x_test, y_test, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# Procesing data before giving to the model\n",
    "#----------------------------------------------------------------------    \n",
    "def normalize_data(x_train, x_test):\n",
    "    normalize = Normalization()\n",
    "    normalize.adapt(x_train)\n",
    "    x_train = np.asarray(normalize(x_train))\n",
    "    print('var_normalized_train : %.10f' % np.var(x_train))\n",
    "    print('mean_normalized_train : %.10f' % np.mean(x_train))\n",
    "    normalize.adapt(x_test)\n",
    "    x_test = np.asarray(normalize(x_test))\n",
    "    print('var_normalized_test : %.10f' % np.var(x_test))\n",
    "    print('mean_normalized_test : %.10f' % np.mean(x_test))\n",
    "        \n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test = normalize_data(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate\n",
    "lr_rate = 0.0001\n",
    "# optimize used for classification\n",
    "opt = Adam(learning_rate=lr_rate)\n",
    "loss = 'categorical_crossentropy'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(loss=loss, optimizer=opt, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BA0CVD0E5uWS",
    "outputId": "d4ae07a3-1103-4311-a366-c6a68258ffbb"
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# model training\n",
    "#----------------------------------------------------------------------  \n",
    "# validation set is taken from train data\n",
    "history = model.fit(x_train, y_train, batch_size=100, epochs=100, shuffle=True, validation_split=0.2, verbose=1)\n",
    "\n",
    "# if there is a validation set\n",
    "# history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=100, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_path = 'testing_models\\my_model_00'\n",
    "model.save(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save:\n",
    "f = open('testing_models\\history_saved_416.pckl', 'wb')\n",
    "pickle.dump(history.history, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loadind saved model and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = keras.models.load_model('testing_models\\my_model_423')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve:    \n",
    "f = open('testing_models\\history_saved_423.pckl', 'rb')\n",
    "history = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9lRYzvjn8p9l"
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------\n",
    "# model evaluation on test data and measure on accuracy metrics\n",
    "#----------------------------------------------------------------------  \n",
    "score = loaded_model.evaluate(x_test, y_test, batch_size=100, verbose=1)\n",
    "print(\"Test Score:\", score[0])\n",
    "print(\"Accuracy: %.2f%%\" % (score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ploting curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1G7DYeEuwGSD"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'], loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train','validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Res5-5l47xME"
   },
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytXoZcsY7N21"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_test, y_scores, classNames):\n",
    "    y_test=np.argmax(y_test, axis=1)\n",
    "    y_scores=np.argmax(y_scores, axis=1)\n",
    "    classes = len(classNames)\n",
    "    cm = confusion_matrix(y_test, y_scores)\n",
    "    print('** Confusion Matrix **')\n",
    "    # print(\"Classification Report\")\n",
    "    # print(classification_report(y_test, y_scores, target_names=classNames))\n",
    "    con = np.zeros((classes,classes))\n",
    "    for x in range(classes):\n",
    "        for y in range(classes):\n",
    "            con[x,y] = cm[x,y]/np.sum(cm[x,:])\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.set(font_scale=0.8) # for label size\n",
    "    df = sns.heatmap(con, annot=True,fmt='.1f', cmap='Blues',xticklabels= classNames , yticklabels= classNames)\n",
    "#     df.figure.savefig(\"Results\\GRU_W_with_X_train(no feature scaling)\\confusion matrix\\confusion_bi_124.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classNames = ['0','1','2','3','4','5','6','7','8','9','10',\n",
    "                  '11','12','13','14','15','16','17','18','19',\n",
    "                  '20','21','22','23','24','25','26','27','28','29']  \n",
    "\n",
    "predictions = loaded_model.predict(x_test)\n",
    "\n",
    "plot_confusion_matrix(y_test, predictions, classNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Concat_ API_MODEL_train_test_split.ipynb",
   "provenance": [
    {
     "file_id": "1tEQzu5qf4jSccPD0zACYpxHdCLNCrjr_",
     "timestamp": 1634925878983
    },
    {
     "file_id": "1R7K9zV4JgE1E9YU89eSxVA3f10KmT9gr",
     "timestamp": 1634839721642
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
